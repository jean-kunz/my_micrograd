{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from graphviz import Digraph\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=[14. 32.], _op=@, label="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "TTensor = TypeVar(\"TTensor\", bound=\"Tensor\")\n",
    "\n",
    "class Tensor:\n",
    "    \n",
    "    def __init__(self, data:list, _prev:set=(), _op: str='', label='') -> None:\n",
    "        self.data = np.array(data)\n",
    "        self._prev = _prev\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __repr__(self) -> str:        \n",
    "        return f\"Value(data={np.array_str(self.data)}, _op={self._op}, label={self.label}\"\n",
    "    \n",
    "    def __add__(self, other: TTensor) -> TTensor:\n",
    "       res = Tensor(self.data + other.data, _op='+', _prev=(self, other))\n",
    "       return res  \n",
    "       \n",
    "    def __mul__(self, other: TTensor) -> TTensor:\n",
    "        return Tensor(self.data * other.data, _op='*', _prev=(self, other))\n",
    "        \n",
    "    def __matmul__(self, other: TTensor) -> TTensor:\n",
    "        return Tensor(self.data @ other.data, _op='@', _prev=(self, other))\n",
    "    \n",
    "    def __pow__(self, other: TTensor) -> TTensor:\n",
    "        return Tensor(self.data ** other.data, _op='**', _prev=(self, other)) \n",
    "    \n",
    "    def __truediv__(self, other: TTensor) -> TTensor:\n",
    "        # /\n",
    "        ...\n",
    "        \n",
    "    def __rmul__(self, other: TTensor) -> TTensor: # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other: TTensor) -> TTensor: # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self: TTensor) -> TTensor: # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __radd__(self, other: TTensor) -> TTensor: # other + self\n",
    "        return self + other        \n",
    "        \n",
    "\n",
    "A = Tensor([[1.,2.,3.],[4.,5.,6.]])\n",
    "B = Tensor([1.,2.,3.])\n",
    "C = (A @ B)\n",
    "D=C**Tensor(2)\n",
    "C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.),\n",
       " tensor([[2., 4., 6.],\n",
       "         [2., 4., 6.]]),\n",
       " tensor([[5., 5.],\n",
       "         [7., 7.],\n",
       "         [9., 9.]]),\n",
       " tensor([[1., 1.],\n",
       "         [1., 1.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "At = torch.tensor([[1.,2.,3.],[4.,5.,6.]], requires_grad=True)\n",
    "Bt = torch.tensor([[1.,1.],[2.,2.],[3.,3.]], requires_grad=True)\n",
    "\n",
    "Ct=At@Bt\n",
    "Ct.retain_grad()\n",
    "#Ct.backward()\n",
    "St = Ct.sum()\n",
    "St.retain_grad()\n",
    "St.backward()\n",
    "St.grad, At.grad, Bt.grad, Ct.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "x2 0.5000001283844369\n",
      "w2 0.0\n",
      "x1 -1.5000003851533106\n",
      "w1 1.0000002567688737\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "#print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print('---')\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrograd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0817e48aad823e29ab0979a99cf84a8de159f5c28af964a8e0018065f64daf7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
