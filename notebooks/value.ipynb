{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value based computations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "from graphviz import Digraph\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root, format='svg', rankdir='LR'):\n",
    "    \"\"\"\n",
    "    format: png | svg | ...\n",
    "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
    "    \"\"\"\n",
    "    assert rankdir in ['LR', 'TB']\n",
    "    nodes, edges = trace(root)\n",
    "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
    "    \n",
    "    for n in nodes:\n",
    "        dot.node(name=str(id(n)), label = \"{%s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            dot.node(name=str(id(n)) + n._op, label=n._op)\n",
    "            dot.edge(str(id(n)) + n._op, str(id(n)))\n",
    "    \n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "    \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class Value:\n",
    "    \n",
    "    def __init__(self, data: float, _prev:set=(), _op: str='', label='') -> None:\n",
    "        self.data = data\n",
    "        self._prev = _prev\n",
    "        self._backward = lambda: None\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        self.grad: float = 0.\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value({self.data = }, {self.grad=}, {self.label = })\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else (Value(other))\n",
    "        out = Value(self.data + other.data, _prev=(self,other),_op='+')\n",
    "        \n",
    "        def _backward():\n",
    "            # we do += to manage the case where we add with itself and it overrides the gradient. So we always add grad. \n",
    "            self.grad += 1. * out.grad\n",
    "            other.grad += 1. * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self *-1\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "        \n",
    "    def __rsub__(self, other):\n",
    "        return self.__sub__(other)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else (Value(other))\n",
    "        out =  Value(self.data * other.data, _prev=(self,other),_op='*')\n",
    "    \n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * (other**-1)\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"Other should be either int or float\"\n",
    "        out = Value(self.data**other, _prev=(self,), _op='**')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other * (self.data**(other-1)) * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), _prev=(self,), _op='exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "        \n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        n = (1-math.exp(-2*x))/(1+math.exp(-2*x))\n",
    "        out = Value(n, _prev=(self,), _op='tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (1-n**2) * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(node: Value):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for prev in node._prev:\n",
    "                    build_topo(prev)\n",
    "                topo.append(node)\n",
    "\n",
    "        build_topo(self) \n",
    "        topo  \n",
    "        self.grad = 1.\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(2., label='a')\n",
    "b = Value(4., label='b')\n",
    "#a+1\n",
    "#a*2\n",
    "2*a\n",
    "#1*a\n",
    "#1+a\n",
    "#a.exp()\n",
    "#a/b\n",
    "c=a-b; c.label='c'\n",
    "\n",
    "c.backward()\n",
    "draw_dot(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Value(2., label='x1')\n",
    "x2 = Value(0., label='x2')\n",
    "\n",
    "w1 = Value(-3., label='w1')\n",
    "w2 = Value(1, label='w2')\n",
    "\n",
    "x1w1 = x1 * w1; x1w1.label='x1w1'\n",
    "x2w2 = x2 * w2; x2w2.label='x2w2'\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "x1w1x2w2= x1w1 + x2w2; x1w1x2w2.label='x1w1x2w2'\n",
    "n = x1w1x2w2+b; n.label='n'\n",
    "e = (2*n).exp()\n",
    "o = (e-1)/(e+1)\n",
    "#o = n.tanh()\n",
    "o.label = 'o'\n",
    "\n",
    "\n",
    "#o.grad = 1.\n",
    "#o._backward()\n",
    "#n._backward()\n",
    "#x1w1x2w2._backward()\n",
    "#x1w1._backward()\n",
    "##b._backward()\n",
    "#x2w2._backward()\n",
    "#x2._backward()\n",
    "o.backward()\n",
    "draw_dot(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0.\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(Module):\n",
    "    \n",
    "    def __init__(self, nin:int, activation='tanh') -> None:\n",
    "        self.w = [Value(random.uniform(-1,1))for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "        self.activation = activation\n",
    "        \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        out = sum([wi*xi for wi,xi in zip(self.w, x)], self.b)\n",
    "        if self.activation=='tanh':\n",
    "            out = out.tanh()\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "        \n",
    "    \n",
    "x =[2., 3., -1.]\n",
    "n = Neuron(3)\n",
    "n(x)\n",
    "n.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(Module):\n",
    "    \n",
    "    def __init__(self, nin, nout) -> None:\n",
    "        self.neurons = [Neuron(nin)for _ in range(nout)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs)==1 else outs\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "    \n",
    "l = Layer(3,4)\n",
    "l(x)\n",
    "l.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "    \n",
    "    def __init__(self, nin, nouts) -> None:\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) \n",
    "                       for i \n",
    "                       in range(len(nouts))]\n",
    "        \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for l in self.layers for p in l.parameters()]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MLP(3,[4,4,1])\n",
    "out = m(x)\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred =[m(x) for x in xs]\n",
    "ypred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr = 0.1\n",
    "epoch_nb = 100\n",
    "for k in range(epoch_nb):\n",
    "    # forward pass\n",
    "    ypred =[m(x) for x in xs]\n",
    "    loss = sum([(ygt-yout)**2 for ygt, yout in zip(ys, ypred)])\n",
    "      \n",
    "    \n",
    "    # backward pass\n",
    "    # zero out grads because during backward we do a += so we accumulate past gradients. We must flush gradient to 0.\n",
    "    m.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    lr = 1-0.9*k/epoch_nb\n",
    "    \n",
    "    # parameter update\n",
    "    for p in m.parameters():\n",
    "        p.data += -lr*p.grad\n",
    "     \n",
    "    #print(k, loss.data, lr)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make up a dataset\n",
    "\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "X, y = make_moons(n_samples=100, noise=0.1)\n",
    "\n",
    "y = y*2 - 1 # make y be -1 or 1\n",
    "# visualize in 2D\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=20, cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model \n",
    "model = MLP(2, [16,16, 1]) # 2-layer neural network\n",
    "print(model)\n",
    "print(\"number of parameters\", len(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def compute_loss(batch_size=None):\n",
    "    \n",
    "    # inline DataLoader :)\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X, y\n",
    "    else:\n",
    "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "        Xb, yb = X[ri], y[ri]\n",
    "    inputs = [list(map(Value, xrow)) for xrow in Xb]\n",
    "    \n",
    "    # forward the model to get scores\n",
    "    scores = list(map(model, inputs))\n",
    "    \n",
    "    # essayer avec autre fonction de loss\n",
    "    \n",
    "    # svm \"max-margin\" loss\n",
    "    losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]\n",
    "    #print(\"sum/len losses\",sum(losses)/len(losses), sum(losses), len(losses))\n",
    "    data_loss = sum(losses)* (1.0 / len(losses))\n",
    "    # L2 regularization\n",
    "    alpha = 1e-4\n",
    "    reg_loss = alpha * sum((p*p for p in model.parameters()))\n",
    "    total_loss = data_loss + reg_loss\n",
    "    \n",
    "    # also get accuracy\n",
    "    accuracy = [(yi > 0) == (scorei.data > 0) for yi, scorei in zip(yb, scores)]\n",
    "    return total_loss, sum(accuracy) / len(accuracy)\n",
    "\n",
    "total_loss, acc = compute_loss()\n",
    "print(total_loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr = 0.1\n",
    "epoch_nb = 80\n",
    "loss_history =[]\n",
    "acc_history = []\n",
    "for k in range(epoch_nb):\n",
    "    loss, acc = compute_loss()\n",
    "    \n",
    "    # backward pass\n",
    "    # zero out grads because during backward we do a += so we accumulate past gradients. We must flush gradient to 0.\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    loss_history.append(loss.data)\n",
    "    acc_history.append(acc)\n",
    "    \n",
    "    learning_rate = 1.0 - 0.9*k/100\n",
    "    for p in model.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "    \n",
    "    if k % 10 == 0:\n",
    "        print(f\"step {k} loss {loss.data}, accuracy {acc*100}%, lr: {learning_rate}\")\n",
    "     \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "#plt.plot(acc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_history)\n",
    "#acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize decision boundary\n",
    "h = 0.25\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "inputs = [list(map(Value, xrow)) for xrow in Xmesh]\n",
    "scores = list(map(model, inputs))\n",
    "Z = np.array([s.data > 0 for s in scores])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('micrograd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "0817e48aad823e29ab0979a99cf84a8de159f5c28af964a8e0018065f64daf7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
