{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value based computations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "from graphviz import Digraph\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root, format='svg', rankdir='LR'):\n",
    "    \"\"\"\n",
    "    format: png | svg | ...\n",
    "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
    "    \"\"\"\n",
    "    assert rankdir in ['LR', 'TB']\n",
    "    nodes, edges = trace(root)\n",
    "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
    "    \n",
    "    for n in nodes:\n",
    "        dot.node(name=str(id(n)), label = \"{%s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            dot.node(name=str(id(n)) + n._op, label=n._op)\n",
    "            dot.edge(str(id(n)) + n._op, str(id(n)))\n",
    "    \n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "    \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class Value:\n",
    "    \n",
    "    def __init__(self, data: float, _prev:set=(), _op: str='', label='') -> None:\n",
    "        self.data = data\n",
    "        self._prev = _prev\n",
    "        self._backward = lambda: None\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        self.grad: float = 0.\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Value({self.data = }, {self.grad=}, {self.label = })\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else (Value(other))\n",
    "        out = Value(self.data + other.data, _prev=(self,other),_op='+')\n",
    "        \n",
    "        def _backward():\n",
    "            # we do += to manage the case where we add with itself and it overrides the gradient. So we always add grad. \n",
    "            self.grad += 1. * out.grad\n",
    "            other.grad += 1. * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self *-1\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "        \n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else (Value(other))\n",
    "        out =  Value(self.data * other.data, _prev=(self,other),_op='*')\n",
    "    \n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * (other**-1)\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"Other should be either int or float\"\n",
    "        out = Value(self.data**other, _prev=(self,), _op='**')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other * (self.data**(other-1)) * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), _prev=(self,), _op='exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        n = (1-math.exp(-2*x))/(1+math.exp(-2*x))\n",
    "        out = Value(n, _prev=(self,), _op='tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (1-n**2) * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(node: Value):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for prev in node._prev:\n",
    "                    build_topo(prev)\n",
    "                topo.append(node)\n",
    "\n",
    "        build_topo(self) \n",
    "        topo  \n",
    "        self.grad = 1.\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(2., label='a')\n",
    "b = Value(4., label='b')\n",
    "#a+1\n",
    "#a*2\n",
    "2*a\n",
    "#1*a\n",
    "#1+a\n",
    "#a.exp()\n",
    "#a/b\n",
    "c=a-b; c.label='c'\n",
    "\n",
    "c.backward()\n",
    "draw_dot(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Value(2., label='x1')\n",
    "x2 = Value(0., label='x2')\n",
    "\n",
    "w1 = Value(-3., label='w1')\n",
    "w2 = Value(1, label='w2')\n",
    "\n",
    "x1w1 = x1 * w1; x1w1.label='x1w1'\n",
    "x2w2 = x2 * w2; x2w2.label='x2w2'\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "x1w1x2w2= x1w1 + x2w2; x1w1x2w2.label='x1w1x2w2'\n",
    "n = x1w1x2w2+b; n.label='n'\n",
    "e = (2*n).exp()\n",
    "o = (e-1)/(e+1)\n",
    "#o = n.tanh()\n",
    "o.label = 'o'\n",
    "\n",
    "\n",
    "#o.grad = 1.\n",
    "#o._backward()\n",
    "#n._backward()\n",
    "#x1w1x2w2._backward()\n",
    "#x1w1._backward()\n",
    "##b._backward()\n",
    "#x2w2._backward()\n",
    "#x2._backward()\n",
    "o.backward()\n",
    "draw_dot(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \n",
    "    def __init__(self, nin:int, activation='tanh') -> None:\n",
    "        self.w = [Value(random.uniform(-1,1))for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "        self.activation = activation\n",
    "        \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        out = sum([wi*xi for wi,xi in zip(self.w, x)], self.b)\n",
    "        if self.activation=='tanh':\n",
    "            out = out.tanh()\n",
    "        return out\n",
    "        \n",
    "    \n",
    "x =[2., 3., -1.]\n",
    "n = Neuron(3)\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self, nin, nout) -> None:\n",
    "        self.neurons = [Neuron(nin)for _ in range(nout)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out\n",
    "    \n",
    "l = Layer(3,4)\n",
    "l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \n",
    "    def __init__(self, nin, nouts) -> None:\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) \n",
    "                       for i \n",
    "                       in range(len(nouts))]\n",
    "        \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "        \n",
    "m = MLP(3,[4,4,1])\n",
    "out = m(x)\n",
    "out[0].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(3., label='a')\n",
    "b = a + a; b.label = 'b'\n",
    "\n",
    "b.backward()    \n",
    "draw_dot(b)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(-2., label='a')\n",
    "b = Value(3., label='b')\n",
    "d = a*b; d.label='d'\n",
    "e = a+b; e.label='e'\n",
    "f = d*e; f.label='f'\n",
    "\n",
    "f.backward()\n",
    "draw_dot(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def fit_polynomial(D, x, y):\n",
    "    # Broadcasting magic\n",
    "    X = x[:, None] ** torch.arange(0, D + 1)[None]\n",
    "    # Least square solution\n",
    "    return torch.linalg.lstsq(X, y).solution\n",
    "\n",
    "D, N = 4, 100\n",
    "x = torch.linspace(-math.pi, math.pi, N)\n",
    "y = x.sin()\n",
    "alpha = fit_polynomial(D, x, y)\n",
    "X = x[:, None] ** torch.arange(0, D + 1)[None]\n",
    "y_hat = X @ alpha\n",
    "#for k in range(N):\n",
    "#    print(x[k].item(), y[k].item(), y_hat[k].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('micrograd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "0817e48aad823e29ab0979a99cf84a8de159f5c28af964a8e0018065f64daf7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
